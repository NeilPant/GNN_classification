# GNN_classification
i've used Social circles: Facebook dataset from https://snap.stanford.edu to classify users into 3 categories(active/passive/inactive) using a GNN. ive also commented on my thinking and how the code works in the main.py file.
UPDATE:
Overview
Built a compact GCN in PyTorch Geometric to classify nodes as Active/Passive/Inactive on the Egoâ€‘Facebook graph. ğŸ§ ğŸ“Š

Achieved up to ~88% accuracy with a reproducible, hardwareâ€‘aware training pipeline. âœ…âš¡

Data and features
Loaded an undirected NetworkX graph from facebook_combined.txt (Stanford dataset), remapped nodes to contiguous IDs, and built a bidirectional COO edge_index for PyG Data(x, edge_index, y). ğŸ“¥ğŸ”—

Engineered and standardized five node features to boost signal: normalized degree, clustering coefficient, PageRank, sampled betweenness centrality, and average neighbor degree. ğŸ§®âœ¨

Derived labels from degree buckets to represent activity levels: Active, Passive, Inactive. ğŸ·ï¸ğŸ“ˆ

Model architecture
Stacked three GCNConv layers with BatchNorm, ReLU, and 0.5 dropout for stable, regularized learning. ğŸ§±ğŸ§ª

Disabled add_self_loops since preprocessed edges already handled diagonal connections. ğŸš«ğŸ”

Kept the model lean with hidden size H=32, totaling ~1.5K parameters for fast iteration and deployability. ğŸ’¡âš™ï¸

Training pipeline
Ensured reproducibility via deterministic seeds across libraries and frameworks. ğŸ¯ğŸ”’

Used AdamW with weight decay and cosine annealing LR scheduling to balance speed and generalization. ğŸ§­ğŸ› ï¸

Added optional AMP scaffolding for mixedâ€‘precision training on supported GPUs; included an alternative aot_eager compile path for tracing without Inductor codegen. ğŸš€ğŸ§©

Splits and evaluation
Employed stratified train/test splits to preserve class balance across labels. ğŸ“šâš–ï¸

Tracked epochâ€‘wise loss, train accuracy, and test accuracy to monitor generalization and early stabilization. ğŸ“ˆğŸ“

Observed test accuracy ranging 70â€“88% depending on epoch budget and hyperparameters; a 200â€‘epoch run reached ~88%, while earlier versions were ~70%. ğŸ¯â±ï¸

Overfitting controls
Reduced overfitting using BatchNorm after each hidden GCNConv, ReLU activations, and 0.5 dropout. ğŸ§ªğŸ›¡ï¸

Architectural and data preprocessing choices (e.g., loop handling) further stabilized training. ğŸ§©ğŸ—ï¸

Class distribution
Reported label skew to contextualize metrics: Active 76.2%, Passive 21.9%, Inactive 1.9%. ğŸ“ŠğŸ”

Reproducibility and MLOps
Documented configuration (paths, hyperparameters, seeds) and modularized feature engineering plus train/eval steps. ğŸ“šğŸ§±

Organized code into clear cells: data loading, feature engineering, model, training loop, and evaluation plots for smooth handoff. ğŸ—‚ï¸ğŸ§­

Included loss/accuracy plots and total parameter count to evidence compactness and readiness. ğŸ“‰âœ…

Deployment
Serialized and saved weights to final_optimized_gnn_with_visualization.pth for downstream inference. ğŸ’¾ğŸš€

Provided documentation and structure suitable for integration and deployment pipelines. ğŸ› ï¸ğŸ“¦

Oneâ€‘line summary
A compact, wellâ€‘regularized GCN with engineered graph features delivers up to ~88% accuracy on Egoâ€‘Facebook, with robust reproducibility, monitoring, and deployment readiness. ğŸ§ âœ…


W
